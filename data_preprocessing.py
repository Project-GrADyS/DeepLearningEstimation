"""
Data Preprocessing Module for Distributed Formation Control (Version 2)

This module is responsible for preprocessing the simulation data generated by data_generation.py
and creating a structured NPZ file for deep learning model training.

Key Features:
- Data loading and validation
- Data cleaning and preprocessing
- Data normalization
- Dataset organization in NPZ format

Dependencies:
    - numpy: Numerical computations
    - pandas: Data manipulation
    - tqdm: Progress bar

Author: Laércio Lucchesi
Date: 2025-04-20
Version: 2.0
"""

import numpy as np
import pandas as pd
from typing import Tuple, Dict, List
import os
import glob
import re
from tqdm import tqdm

# Dataset type constants
TRAIN, VAL, TEST = 0, 1, 2

def parse_filename(filename: str) -> Dict[str, float]:
    """
    Parse simulation parameters from filename.
    Format: P{size}R{range}D{delay}F{failure}.csv
    
    Args:
        filename (str): Name of the simulation file
        
    Returns:
        Dict[str, float]: Dictionary containing parsed parameters
    """
    pattern = r'P(\d+)R(\d+)D(\d+)F(\d+)\.csv'
    match = re.match(pattern, filename)
    if not match:
        raise ValueError(f"Invalid filename format: {filename}")
    
    size, range_, delay, failure = map(int, match.groups())
    return {
        'size_formation': size,
        'comm_range': range_,
        'comm_delay': delay / 1000,  # Convert from ms to seconds
        'comm_failure': failure / 100  # Convert from percentage to decimal
    }

def load_data(dataset_type: int) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
    """
    Load simulation data from CSV files based on dataset type.
    
    Args:
        dataset_type (int): Type of dataset to load (TRAIN=0, VAL=1, TEST=2)
        
    Returns:
        Tuple[Dict[str, np.ndarray], np.ndarray]: Dictionary of arrays and timestamps array
    """
    # Map dataset type to directory name
    dataset_dir_map = {TRAIN: "dataset_training", VAL: "dataset_validation", TEST: "dataset_test"}
    dataset_dir = dataset_dir_map[dataset_type]
    
    if not os.path.exists(dataset_dir):
        raise FileNotFoundError(f"Dataset directory not found: {dataset_dir}")
    
    # Initialize lists to store data
    size_formation_list = []
    comm_delay_list = []
    comm_range_list = []
    comm_failure_list = []
    shape_error_list = []
    presence_error_list = []
    leader_off_list = []
    timestamps_list = None  # Initialize as None instead of empty list
    
    # Get list of files to process
    files = glob.glob(os.path.join(dataset_dir, "*.csv"))
    
    # Process each simulation file in the directory with progress bar
    for file_path in tqdm(files, desc=f"Processing {dataset_dir} files", unit="file"):
        filename = os.path.basename(file_path)
        
        # Parse parameters from filename
        params = parse_filename(filename)
        
        # Read simulation data
        sim_data = pd.read_csv(file_path)
        
        # Extract parameters
        size_formation_list.append(params['size_formation'])
        comm_delay_list.append(params['comm_delay'])
        comm_range_list.append(params['comm_range'])
        comm_failure_list.append(params['comm_failure'])
        
        # Extract time series data
        shape_error_list.append(sim_data['shape_error'].values)
        presence_error_list.append(sim_data['presence_error'].values)
        leader_off_list.append(sim_data['leader_off'].values)
        
        # Store timestamps (only need to do this once as they're the same for all files)
        if timestamps_list is None:
            timestamps_list = sim_data['timestamp'].values
    
    # Convert lists to numpy arrays
    data_dict = {
        'size_formation': np.array(size_formation_list),
        'comm_delay': np.array(comm_delay_list),
        'comm_range': np.array(comm_range_list),
        'comm_failure': np.array(comm_failure_list),
        'shape_error': np.array(shape_error_list),
        'presence_error': np.array(presence_error_list),
        'leader_off': np.array(leader_off_list)
    }
    
    timestamps = timestamps_list  # No need to convert to array as it's already an array
    
    return data_dict, timestamps

def add_noise_to_delay(comm_delay: np.ndarray) -> np.ndarray:
    """
    Add ±1% of the original delay value as random noise, ensuring they remain positive.
    
    Args:
        comm_delay (np.ndarray): Array of communication delay values
        
    Returns:
        np.ndarray: Array with noisy delay values
    """
    # Calculate 1% of each delay value
    noise_range = comm_delay * 0.01
    
    # Generate random noise between -noise_range and +noise_range for each value
    noise = np.random.uniform(-1, 1, size=len(comm_delay)) * noise_range
    
    # Apply noise to delay values
    noisy_delay = comm_delay + noise
    
    # Ensure all values are positive
    noisy_delay = np.maximum(noisy_delay, 0.0)  # Allows only positive values
    
    return noisy_delay

def normalize_columns(data_dict: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
    """
    Normalize specified columns by dividing each value by its maximum value.
    
    Args:
        data_dict (Dict[str, np.ndarray]): Dictionary of data arrays
        
    Returns:
        Dict[str, np.ndarray]: Dictionary with normalized arrays
    """
    # Columns to normalize
    columns_to_normalize = ['size_formation', 'comm_delay', 'comm_range', 'comm_failure']
    
    # Create a copy to avoid modifying the original data
    normalized_data = data_dict.copy()
    
    # Print normalization factors (max values)
    print("\nNormalization factors (max values):")
    for column in columns_to_normalize:
        max_value = normalized_data[column].max()
        normalized_data[column] = normalized_data[column] / max_value
        print(f"{column}: {max_value}")
    
    return normalized_data

def main():
    """
    Main function to execute the data preprocessing pipeline.
    """
    print("Starting data preprocessing (Version 2)...")
    
    # Load data from all dataset types
    all_data = {}
    all_timestamps = None
    
    # Process each dataset
    for dataset_type in [TRAIN, VAL, TEST]:
        dataset_name = {TRAIN: "dataset_training", VAL: "dataset_validation", TEST: "dataset_test"}[dataset_type]
        print(f"\nProcessing {dataset_name} dataset...")
        
        # Load data for this dataset type
        data_dict, timestamps = load_data(dataset_type)
        
        # Store timestamps (only need to do this once)
        if all_timestamps is None:
            all_timestamps = timestamps
        
        # Add dataset type to the data
        data_dict['dataset_type'] = np.full(len(data_dict['size_formation']), dataset_type)
        
        # Add to the combined dataset
        for key, value in data_dict.items():
            if key not in all_data:
                all_data[key] = value
            else:
                all_data[key] = np.concatenate([all_data[key], value])
    
    # Add noise to communication delay
    print("\nAdding noise to communication delay...")
    all_data['comm_delay'] = add_noise_to_delay(all_data['comm_delay'])
    
    # Normalize specified columns
    print("\nNormalizing columns...")
    all_data = normalize_columns(all_data)
    
    # Save the combined dataset as NPZ file
    print("\nSaving dataset as NPZ file...")
    np.savez("dataset.npz",
             dataset_type=all_data['dataset_type'],
             size_formation=all_data['size_formation'],
             comm_delay=all_data['comm_delay'],
             shape_error=all_data['shape_error'],
             presence_error=all_data['presence_error'],
             leader_off=all_data['leader_off'],
             comm_range=all_data['comm_range'],
             comm_failure=all_data['comm_failure'],
             timestamps=all_timestamps)
    
    print("Dataset successfully created and saved as 'dataset.npz'")
    
    # Print dataset statistics
    print("\nDataset statistics:")
    print(f"Total samples: {len(all_data['dataset_type'])}")
    print(f"Training samples: {np.sum(all_data['dataset_type'] == TRAIN)}")
    print(f"Validation samples: {np.sum(all_data['dataset_type'] == VAL)}")
    print(f"Test samples: {np.sum(all_data['dataset_type'] == TEST)}")
    print(f"Time steps: {len(all_timestamps)}")
    
    # Load and display first 10 samples from the saved dataset
    print("\nFirst 10 samples from dataset.npz:")
    dataset = np.load("dataset.npz")
    
    # Create a formatted display of the first 10 samples
    print("\nSample  | Dataset Type | Size Formation | Comm Delay | Comm Range | Comm Failure")
    print("-" * 75)
    for i in range(min(10, len(dataset['dataset_type']))):
        print(f"{i:7d} | {dataset['dataset_type'][i]:11d} | {dataset['size_formation'][i]:13.3f} | {dataset['comm_delay'][i]:10.3f} | {dataset['comm_range'][i]:10.3f} | {dataset['comm_failure'][i]:12.3f}")
    
    # Show shapes of time series data
    print("\nTime series data shapes:")
    print(f"Shape Error: {dataset['shape_error'][:10].shape}")
    print(f"Presence Error: {dataset['presence_error'][:10].shape}")
    print(f"Leader Off: {dataset['leader_off'][:10].shape}")
    print(f"Timestamps: {dataset['timestamps'].shape}")
    
    # Close the dataset
    dataset.close()

if __name__ == "__main__":
    main() 