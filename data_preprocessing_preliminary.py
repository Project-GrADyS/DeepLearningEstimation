"""
Data Preprocessing Module for Distributed Formation Control

This module is responsible for preprocessing the simulation data generated by data_generation.py.
It handles data cleaning, normalization, and preparation for training machine learning models.

Key Features:
- Data loading and validation
- Data cleaning and preprocessing
- Data normalization
- Dataset splitting and organization

Dependencies:
    - numpy: Numerical computations
    - pandas: Data manipulation
    - scikit-learn: Data preprocessing tools
    - tqdm: Progress bar

Author: Laércio Lucchesi
Date: 2025-03-30
Version: 1.0
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from typing import Tuple, Dict, List
import os
import glob
import re
from tqdm import tqdm

def parse_filename(filename: str) -> Dict[str, float]:
    """
    Parse simulation parameters from filename.
    Format: P{size}R{range}D{delay}F{failure}.csv
    
    Args:
        filename (str): Name of the simulation file
        
    Returns:
        Dict[str, float]: Dictionary containing parsed parameters
    """
    pattern = r'P(\d+)R(\d+)D(\d+)F(\d+)\.csv'
    match = re.match(pattern, filename)
    if not match:
        raise ValueError(f"Invalid filename format: {filename}")
    
    size, range_, delay, failure = map(int, match.groups())
    return {
        'size_formation': size,
        'comm_range': range_,
        'comm_delay': delay / 1000,  # Convert from ms to seconds
        'comm_failure': failure / 100  # Convert from percentage to decimal
    }

def calculate_time_windows(data: pd.DataFrame) -> Dict[str, float]:
    """
    Calculate average shape error for different time windows.
    
    Args:
        data (pd.DataFrame): Simulation data with timestamp, shape_error, leader_off, and presence_error columns
        
    Returns:
        Dict[str, float]: Dictionary containing average shape errors for different time windows
    """
    # Ensure timestamp is in seconds
    data['timestamp'] = data['timestamp'].astype(float)
    
    return {
        'shape_error_avg_030s': data[data['timestamp'] >= data['timestamp'].max() - 30]['shape_error'].mean(),
        'shape_error_avg_1min': data[data['timestamp'] >= data['timestamp'].max() - 60]['shape_error'].mean(),
        'shape_error_avg_2min': data[data['timestamp'] >= data['timestamp'].max() - 120]['shape_error'].mean(),
        'shape_error_avg_3min': data[data['timestamp'] >= data['timestamp'].max() - 180]['shape_error'].mean(),
        'shape_error_avg_4min': data[data['timestamp'] >= data['timestamp'].max() - 240]['shape_error'].mean(),
        'shape_error_avg_5min': data[data['timestamp'] >= data['timestamp'].max() - 300]['shape_error'].mean(),
        'leader_off_avg_030s': data[data['timestamp'] >= data['timestamp'].max() - 30]['leader_off'].mean(),
        'leader_off_avg_1min': data[data['timestamp'] >= data['timestamp'].max() - 60]['leader_off'].mean(),
        'leader_off_avg_2min': data[data['timestamp'] >= data['timestamp'].max() - 120]['leader_off'].mean(),
        'leader_off_avg_3min': data[data['timestamp'] >= data['timestamp'].max() - 180]['leader_off'].mean(),
        'leader_off_avg_4min': data[data['timestamp'] >= data['timestamp'].max() - 240]['leader_off'].mean(),
        'leader_off_avg_5min': data[data['timestamp'] >= data['timestamp'].max() - 300]['leader_off'].mean(),
        'presence_error_avg_030s': data[data['timestamp'] >= data['timestamp'].max() - 30]['presence_error'].mean(),
        'presence_error_avg_1min': data[data['timestamp'] >= data['timestamp'].max() - 60]['presence_error'].mean(),
        'presence_error_avg_2min': data[data['timestamp'] >= data['timestamp'].max() - 120]['presence_error'].mean(),
        'presence_error_avg_3min': data[data['timestamp'] >= data['timestamp'].max() - 180]['presence_error'].mean(),
        'presence_error_avg_4min': data[data['timestamp'] >= data['timestamp'].max() - 240]['presence_error'].mean(),
        'presence_error_avg_5min': data[data['timestamp'] >= data['timestamp'].max() - 300]['presence_error'].mean()
    }

def load_data(dataset_type: str) -> pd.DataFrame:
    """
    Load simulation data from CSV files based on dataset type.
    
    Args:
        dataset_type (str): Type of dataset to load ('training', 'validation', or 'test')
        
    Returns:
        pd.DataFrame: Combined dataset from all simulation files
    """
    dataset_dir = f"dataset_{dataset_type}"
    if not os.path.exists(dataset_dir):
        raise FileNotFoundError(f"Dataset directory not found: {dataset_dir}")
    
    all_data = []
    
    # Get list of files to process
    files = glob.glob(os.path.join(dataset_dir, "*.csv"))
    
    # Process each simulation file in the directory with progress bar
    for file_path in tqdm(files, desc=f"Processing {dataset_type} files", unit="file"):
        filename = os.path.basename(file_path)
        
        # Parse parameters from filename
        params = parse_filename(filename)
        
        # Read simulation data
        sim_data = pd.read_csv(file_path)
        
        # Calculate time window averages
        time_windows = calculate_time_windows(sim_data)
        
        # Combine all data
        row_data = {
            'dataset_type': dataset_type,
            **params,
            **time_windows
        }
        all_data.append(row_data)
    
    return pd.DataFrame(all_data)

def preprocess_data(data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    """
    Preprocess the loaded data for machine learning.
    
    Args:
        data (pd.DataFrame): Raw simulation data
        
    Returns:
        Tuple[np.ndarray, np.ndarray]: Preprocessed features and labels
    """
    pass

def add_noise_to_delay(data: pd.DataFrame) -> pd.DataFrame:
    """
    Add ±1% of the original delay value as random noise, ensuring they remain positive.
    
    Args:
        data (pd.DataFrame): Input dataset
        
    Returns:
        pd.DataFrame: Dataset with noisy delay values
    """
    # Calculate 1% of each delay value
    noise_range = data['comm_delay'] * 0.01
    
    # Generate random noise between -noise_range and +noise_range for each value
    noise = np.random.uniform(-1, 1, size=len(data)) * noise_range
    
    # Apply noise to delay values
    noisy_delay = data['comm_delay'] + noise
    
    # Ensure all values are positive
    noisy_delay = np.maximum(noisy_delay, 0.0)  # Allows only positive values
    
    # Update the delay column
    data['comm_delay'] = noisy_delay
    
    return data

def normalize_columns(data: pd.DataFrame) -> pd.DataFrame:
    """
    Normalize specified columns by dividing each value by its maximum value.
    
    Args:
        data (pd.DataFrame): Input dataset
        
    Returns:
        pd.DataFrame: Dataset with normalized columns
    """
    # Columns to normalize
    columns_to_normalize = ['size_formation', 'comm_delay', 'comm_range', 'comm_failure']
    
    # Create a copy to avoid modifying the original data
    normalized_data = data.copy()
    
    # Print normalization factors
    print("\nNormalization factors:")
    for column in columns_to_normalize:
        max_value = normalized_data[column].max()
        normalized_data[column] = normalized_data[column] / max_value
        print(f"{column}: {max_value}")
    
    return normalized_data

def main():
    """
    Main function to execute the data preprocessing pipeline.
    """
    print("Starting data preprocessing...")
    
    # Load and combine data from all dataset types
    datasets = ['training', 'validation', 'test']
    all_data = []
    
    # Process each dataset
    for dataset in datasets:
        dataset_data = load_data(dataset)
        all_data.append(dataset_data)
    
    # Combine all datasets
    all_data = pd.concat(all_data, ignore_index=True)
    
    # Add noise to communication delay
    all_data = add_noise_to_delay(all_data)
    
    # Define the desired column order
    column_order = [
        'dataset_type',
        'size_formation',
        'comm_delay',  # Moved before comm_range
        'comm_range',  # Moved after comm_delay
        'comm_failure',
        'shape_error_avg_030s',
        'shape_error_avg_1min',
        'shape_error_avg_2min',
        'shape_error_avg_3min',
        'shape_error_avg_4min',
        'shape_error_avg_5min',
        'leader_off_avg_030s',
        'leader_off_avg_1min',
        'leader_off_avg_2min',
        'leader_off_avg_3min',
        'leader_off_avg_4min',
        'leader_off_avg_5min',
        'presence_error_avg_030s',
        'presence_error_avg_1min',
        'presence_error_avg_2min',
        'presence_error_avg_3min',
        'presence_error_avg_4min',
        'presence_error_avg_5min'
    ]
    
    # Reorder columns
    all_data = all_data[column_order]
    
    # Normalize specified columns
    all_data = normalize_columns(all_data)
    
    # Save the combined dataset
    all_data.to_csv('dataset.csv', index=False)
    print(" ")
    print("Dataset successfully created and saved as 'dataset.csv'")

if __name__ == "__main__":
    main() 